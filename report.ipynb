{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba0529ad",
   "metadata": {},
   "source": [
    "# BBM409 : Introduction to Machine Learning Lab. Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38b68d9",
   "metadata": {},
   "source": [
    "#### PROBLEM DEFINITION :\n",
    "\n",
    "    \n",
    "In this assignment, we were asked to understand and familiarize with the decision tree algorithm. We will experiment with decision tree model (by using ID3 algorithm) on the Diabetes Risk Prediction dataset.\n",
    "\n",
    "    \n",
    "In the first part of the experiment, we implemented a decision tree model for predicting whether a patient is a potential diabetic or not. For this experiment, we split our data into two parts. Training and test.\n",
    "\n",
    "    \n",
    "In the second part of the experiment, we are expected to preform pruning operation to the twigs of the tree that we created at the first part. But we could not implement pruning for our decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "fb3a1cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib==3.5.0 in c:\\users\\berkay\\desktop\\workspace\\bbm409\\assig_2\\venv\\lib\\site-packages (from -r requirements.txt (line 1)) (3.5.0)\n",
      "Requirement already satisfied: numpy==1.21.4 in c:\\users\\berkay\\desktop\\workspace\\bbm409\\assig_2\\venv\\lib\\site-packages (from -r requirements.txt (line 2)) (1.21.4)\n",
      "Requirement already satisfied: pandas==1.3.4 in c:\\users\\berkay\\desktop\\workspace\\bbm409\\assig_2\\venv\\lib\\site-packages (from -r requirements.txt (line 3)) (1.3.4)\n",
      "Requirement already satisfied: scikit-learn==1.0.1 in c:\\users\\berkay\\desktop\\workspace\\bbm409\\assig_2\\venv\\lib\\site-packages (from -r requirements.txt (line 4)) (1.0.1)\n",
      "Requirement already satisfied: scipy==1.7.2 in c:\\users\\berkay\\desktop\\workspace\\bbm409\\assig_2\\venv\\lib\\site-packages (from -r requirements.txt (line 5)) (1.7.2)\n",
      "Requirement already satisfied: seaborn==0.11.2 in c:\\users\\berkay\\desktop\\workspace\\bbm409\\assig_2\\venv\\lib\\site-packages (from -r requirements.txt (line 6)) (0.11.2)\n",
      "Requirement already satisfied: setuptools-scm>=4 in c:\\users\\berkay\\desktop\\workspace\\bbm409\\assig_2\\venv\\lib\\site-packages (from matplotlib==3.5.0->-r requirements.txt (line 1)) (6.3.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\berkay\\desktop\\workspace\\bbm409\\assig_2\\venv\\lib\\site-packages (from matplotlib==3.5.0->-r requirements.txt (line 1)) (3.0.6)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\berkay\\desktop\\workspace\\bbm409\\assig_2\\venv\\lib\\site-packages (from matplotlib==3.5.0->-r requirements.txt (line 1)) (4.28.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\berkay\\desktop\\workspace\\bbm409\\assig_2\\venv\\lib\\site-packages (from matplotlib==3.5.0->-r requirements.txt (line 1)) (21.3)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\berkay\\desktop\\workspace\\bbm409\\assig_2\\venv\\lib\\site-packages (from matplotlib==3.5.0->-r requirements.txt (line 1)) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\berkay\\desktop\\workspace\\bbm409\\assig_2\\venv\\lib\\site-packages (from matplotlib==3.5.0->-r requirements.txt (line 1)) (8.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\berkay\\desktop\\workspace\\bbm409\\assig_2\\venv\\lib\\site-packages (from matplotlib==3.5.0->-r requirements.txt (line 1)) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\berkay\\desktop\\workspace\\bbm409\\assig_2\\venv\\lib\\site-packages (from matplotlib==3.5.0->-r requirements.txt (line 1)) (1.3.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\berkay\\desktop\\workspace\\bbm409\\assig_2\\venv\\lib\\site-packages (from pandas==1.3.4->-r requirements.txt (line 3)) (2021.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\berkay\\desktop\\workspace\\bbm409\\assig_2\\venv\\lib\\site-packages (from scikit-learn==1.0.1->-r requirements.txt (line 4)) (3.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\berkay\\desktop\\workspace\\bbm409\\assig_2\\venv\\lib\\site-packages (from scikit-learn==1.0.1->-r requirements.txt (line 4)) (1.1.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\berkay\\desktop\\workspace\\bbm409\\assig_2\\venv\\lib\\site-packages (from setuptools-scm>=4->matplotlib==3.5.0->-r requirements.txt (line 1)) (41.2.0)\n",
      "Requirement already satisfied: tomli>=1.0.0 in c:\\users\\berkay\\desktop\\workspace\\bbm409\\assig_2\\venv\\lib\\site-packages (from setuptools-scm>=4->matplotlib==3.5.0->-r requirements.txt (line 1)) (1.2.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\berkay\\desktop\\workspace\\bbm409\\assig_2\\venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib==3.5.0->-r requirements.txt (line 1)) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 19.2.3, however version 21.3.1 is available.\n",
      "You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d4e429",
   "metadata": {},
   "source": [
    "First, we need to install the packages specified in the requirements.txt file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feaa207",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd145aa4",
   "metadata": {},
   "source": [
    "We imported the necessary packages and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "fa1a9da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0633167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Util Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_confusion_matrix(confusion_matrix):\n",
    "    # heatmap display for confusion matrix\n",
    "    labels = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n",
    "    length = len(max(labels))+4\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    \n",
    "    annots = [f\"{str(label)}({str(value)})\" for array in np.dstack((labels,confusion_matrix)) for (label, value) in array]\n",
    "    annots = np.asarray(annots).reshape(2,2).astype(str)\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sns.heatmap(confusion_matrix, annot=annots, fmt=f\".{length}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column Map Creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_column_map(columns):\n",
    "    # create dictionary as {\"column_index\": \"column_name\", ....}\n",
    "    return {index:columns[index] for index in range(len(list(columns))-1)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation Splitter Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_cross_validation(data, split):\n",
    "    # function for 5 fold cross validation splitter, returns splitted data \n",
    "    kfold = KFold(n_splits=split, shuffle=True,random_state=123456)\n",
    "\n",
    "\n",
    "    splitted = []\n",
    "    for _train_index, _test_index in kfold.split(data):\n",
    "        train, test = data[_train_index], data[_test_index]\n",
    "        splitted.append((train, test))\n",
    "\n",
    "    return splitted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ff0c92",
   "metadata": {},
   "source": [
    "### Encoder of the Age Attribute Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2d127b",
   "metadata": {},
   "source": [
    "Performs discretization operation by encoding age values as 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1d99ce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_age(column, max, min):\n",
    "    # discretizing age as 0 and 1\n",
    "    _range = ((max-min)/2)\n",
    "    \n",
    "    intervals = []\n",
    "\n",
    "    for index in range(1,3):\n",
    "        temp_max = min+_range\n",
    "\n",
    "        intervals.append((int(min), int(temp_max)))\n",
    "        min = temp_max\n",
    "\n",
    "    encoded = []\n",
    "    for age in column:\n",
    "        for index, interval in enumerate(intervals):\n",
    "            __range = range(interval[0],interval[1]+1)\n",
    "            if age in __range:\n",
    "                encoded.append(index)\n",
    "                break\n",
    "    \n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aff9d9fe",
   "metadata": {},
   "source": [
    "### General Encoder Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b3a9ae",
   "metadata": {},
   "source": [
    "Performs Encoding operation for all features (for age, calling *encode_Age* function and 1 for **YEs** and 0 for **NO** on other features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "cbea15d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_features(data):\n",
    "    \"\"\"\n",
    "    encoding all features as 0 and 1(also calls encode_age function)\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # get columns which includes yes or no as a value\n",
    "    binary_columns = data.columns.tolist()\n",
    "    binary_columns.remove(\"Age\")\n",
    "\n",
    "    # Appling binary encoding to 'Age' column\n",
    "    ages = data[\"Age\"].tolist()\n",
    "    max_age = max(ages)\n",
    "    min_age = min(ages)\n",
    "    \n",
    "\n",
    "    encoded_ages = encode_age(ages, max_age, min_age)\n",
    "  \n",
    "    # replace age column with encoded version\n",
    "    data[\"Age\"] = encoded_ages\n",
    "\n",
    "    for col in binary_columns:\n",
    "        values_array = np.unique(data[col].tolist())\n",
    "        if not ((\"Yes\" in values_array[0]) or (\"Positive\" in values_array[0])):\n",
    "            # little configuration for giving 1 for Yes and 0 for No \n",
    "            values_array = values_array[::-1]\n",
    "        \n",
    "        data[col] = data[col].apply(lambda x:1 if x==values_array[0] else 0)\n",
    "\n",
    "    \n",
    "    return data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performs cross validation using splitted data and calculates **precision**, **recall**, **f1**, **accuracy** scores and **confusion matrix** (to be displayed later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(splitted, column_map):\n",
    "    counter = 0\n",
    "    trees = []\n",
    "    confusion_matrices = []\n",
    "    for row in splitted:\n",
    "        _train, _test = row\n",
    "\n",
    "        counter += 1\n",
    "        X_train = _train[:,:-1]\n",
    "        y_train = _train[:,-1].reshape(-1,1)\n",
    "\n",
    "        X_test = _test[:,:-1]\n",
    "        y_test = _test[:,-1].reshape(-1,1)\n",
    "\n",
    "\n",
    "        tree = DecisionTree()\n",
    "        trees.append(tree)\n",
    "    \n",
    "        tree.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "        # using test set\n",
    "        predictions, label = tree.predict(X_test, y_test)\n",
    "\n",
    "        print(\"********************************************************\")\n",
    "        \n",
    "        precision = precision_score(label, predictions)\n",
    "        recall = recall_score(label, predictions)\n",
    "        f1 = f1_score(label, predictions) \n",
    "        accuracy = accuracy_score(label, predictions)\n",
    "        _confusion_matrix = confusion_matrix(label, predictions)\n",
    "        confusion_matrices.append(_confusion_matrix)\n",
    "        print(f\"For Testing --> Fold {counter} -> Precision: {precision}, Recall: {recall}, F1: {f1}, Accuracy: {accuracy}\")\n",
    "\n",
    "        # using training set \n",
    "        tr_predictions, tr_label = tree.predict(X_train, y_train)\n",
    "            \n",
    "        tr_precision = precision_score(tr_label, tr_predictions)\n",
    "        tr_recall = recall_score(tr_label, tr_predictions)\n",
    "        tr_f1 = f1_score(tr_label, tr_predictions) \n",
    "        tr_accuracy = accuracy_score(tr_label, tr_predictions)\n",
    "\n",
    "        print(f\"For Training --> Fold {counter} -> Precision: {tr_precision}, Recall: {tr_recall}, F1: {tr_f1}, Accuracy: {tr_accuracy}\")\n",
    "\n",
    "\n",
    "    return trees, confusion_matrices, column_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c107ba",
   "metadata": {},
   "source": [
    "### Node Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115b3292",
   "metadata": {},
   "source": [
    "Node class that creates each leaf or node of the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "17007adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    # Feature column index\n",
    "    value = None\n",
    "\n",
    "    # previous node's feature value\n",
    "    feature_value = None\n",
    "\n",
    "    # True for leaf nodes, false for other\n",
    "    is_leaf = None\n",
    "\n",
    "    # format -> [feature1, feature2], includes Node instances\n",
    "    children = None\n",
    "\n",
    "    # not None for leaves, for leaves it specifies prediction    \n",
    "    leaf_value = None\n",
    "\n",
    "    def __init__(self, value=None):\n",
    "        self.value = value\n",
    "        self.is_leaf = False\n",
    "        self.children = []\n",
    "    def __str__(self):\n",
    "     return f\"Feature -> {self.value} -> {self.is_leaf}, {len(self.children)}, {self.leaf_value}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff41d1",
   "metadata": {},
   "source": [
    "### DecisionTree Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e264744a",
   "metadata": {},
   "source": [
    "Decision tree class that makes up fundemental decision tree with the specific methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a7053081",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self):\n",
    "        # root node\n",
    "        self.head = Node()\n",
    "\n",
    "    def choose_best_feature(self, _set, features):  # features\n",
    "        # choosing best feature based on entropies and information gains\n",
    "\n",
    "        relative_entropy, rel_num_pos, rel_num_neg = self.calculate_entropy(_set[:, -1])\n",
    "\n",
    "        gains = []\n",
    "        for column in features:\n",
    "            information = 0\n",
    "            for value in np.unique(_set[:, column]):\n",
    "                # filter for attribute value\n",
    "                temp_filter = (_set[:, column] == value)\n",
    "\n",
    "                # using that filter to get attributes\n",
    "                temp_samples = _set[temp_filter]\n",
    "\n",
    "                # calculate entropy and number of samples\n",
    "                temp_entropy, temp_pos, temp_neg = self.calculate_entropy(temp_samples[:, -1])\n",
    "\n",
    "                # calculate ratio of relevant attribute\n",
    "                temp_ratio = (temp_pos + temp_neg) / (rel_num_pos + rel_num_neg)\n",
    "\n",
    "                # summing the result with information\n",
    "                information += (temp_entropy * temp_ratio)\n",
    "\n",
    "            gain = relative_entropy - information\n",
    "\n",
    "            gains.append((column, gain))\n",
    "\n",
    "        gains = sorted(gains, key=lambda x: x[1], reverse=True)\n",
    "        return gains[0]\n",
    "\n",
    "    def labelStatusCheck(self, examples):\n",
    "        # checks whether all given data label is 0 or 1 or else\n",
    "        pos_filter = examples[:, -1] == 1\n",
    "        neg_filter = examples[:, -1] == 0\n",
    "\n",
    "        positives = examples[pos_filter]\n",
    "        negatives = examples[neg_filter]\n",
    "\n",
    "        num_pos = len(positives)\n",
    "        num_neg = len(negatives)\n",
    "\n",
    "        if num_pos == 0:\n",
    "            # all 0\n",
    "            return 0\n",
    "        elif num_neg == 0:\n",
    "            # all 1\n",
    "            return 1\n",
    "        else:\n",
    "            # both exists\n",
    "            return -1\n",
    "\n",
    "    def mostCommonTargetAttribute(self, examples):\n",
    "        # returns most common label in given data\n",
    "        pos_filter = examples[:, -1] == 1\n",
    "        neg_filter = examples[:, -1] == 0\n",
    "\n",
    "        positives = examples[pos_filter]\n",
    "        negatives = examples[neg_filter]\n",
    "\n",
    "        num_pos = len(positives)\n",
    "        num_neg = len(negatives)\n",
    "        return (1 if (num_pos > num_neg) else 0)\n",
    "\n",
    "    def ID3(self, root, data, features):\n",
    "        # ID3 general function, calls itself\n",
    "        if not root:\n",
    "            root = Node()\n",
    "\n",
    "        if self.labelStatusCheck(data) == 1:\n",
    "            # all labels are 1, becomes leaf\n",
    "            root.is_leaf = True\n",
    "            root.leaf_value = 1\n",
    "            return root\n",
    "        elif self.labelStatusCheck(data) == 0:\n",
    "            # all labels are 0, becomes leaf\n",
    "            root.is_leaf = True\n",
    "            root.leaf_value = 0\n",
    "            return root\n",
    "\n",
    "        if len(features) == 0:\n",
    "            # no feature left, leaf value becomes the most common label value\n",
    "            root.is_leaf = True\n",
    "            root.leaf_value = self.mostCommonTargetAttribute(data)\n",
    "            return root\n",
    "\n",
    "        # choose best feature according to its gain\n",
    "        res = self.choose_best_feature(data, features)\n",
    "        best_feature = res[0]\n",
    "        gain = res[1]\n",
    "\n",
    "        root.value = best_feature\n",
    "\n",
    "        chosen_feature_values = np.unique(data[:, best_feature])\n",
    "        for value in chosen_feature_values:\n",
    "            # recursive calls based on children\n",
    "\n",
    "            # configure child node\n",
    "            child = Node()\n",
    "            root.children.append(child)\n",
    "            child.feature_value = value\n",
    "\n",
    "            # filter data according to chosen best feature\n",
    "            _filter = data[:, best_feature] == value\n",
    "            new_data = data[_filter]\n",
    "\n",
    "            # update features (remove chosen feature)\n",
    "            new_features = self.update_features(features, best_feature)\n",
    "\n",
    "            # recursive call\n",
    "            self.ID3(child, new_data, new_features)\n",
    "\n",
    "        return root\n",
    "\n",
    "    def predict(self, X, y):\n",
    "        # prediction method\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for index in range(len(X)):\n",
    "            X_row = X[index]\n",
    "            y_row = y[index]\n",
    "\n",
    "            # calling  traverse function for traversing and returning prediction\n",
    "            pred = self.traverse(self.head, X_row)\n",
    "\n",
    "            # predictions and labels arrays\n",
    "            y_pred.append(pred)\n",
    "            y_true.append(y_row[0])\n",
    "\n",
    "        return y_pred, y_true\n",
    "\n",
    "    def display_tree(self, node, column_map, level=0):\n",
    "        # text based display function for tree\n",
    "        try:\n",
    "            self.display_tree((node.children)[0], column_map, level + 1)\n",
    "        except IndexError:\n",
    "            pass\n",
    "        name = column_map[node.value] if (node.value != None) else node.leaf_value\n",
    "        print(' ' * 4 * level + '->', name)\n",
    "        try:\n",
    "            self.display_tree((node.children)[1], column_map, level + 1)\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "    def update_features(self, features, chosen_best):\n",
    "        # update features according to given chosen feature (remove chosen feature)\n",
    "        updated = np.delete(features, np.where(features == chosen_best), axis=0)\n",
    "        return updated\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # function for trigger tree build\n",
    "\n",
    "        data = np.concatenate((X, y), axis=1)\n",
    "\n",
    "        features = np.array(list(range(len(data[0, :-1]))))\n",
    "\n",
    "        # first call for ID3\n",
    "        self.ID3(self.head, data, features)\n",
    "\n",
    "    def traverse(self, node, X):\n",
    "        # tree traverse function\n",
    "\n",
    "        if node.is_leaf:\n",
    "            # returning prediction\n",
    "            return node.leaf_value\n",
    "\n",
    "        value = node.value\n",
    "        temp_feature_value = X[value]\n",
    "\n",
    "        for child in node.children:\n",
    "            feature_value = child.feature_value\n",
    "            if feature_value == temp_feature_value:\n",
    "                return self.traverse(child, X)\n",
    "\n",
    "        return (node.feature_value + 1) % 2\n",
    "\n",
    "    def calculate_entropy(self, y):\n",
    "        # calculates entropy for given set\n",
    "\n",
    "        num_pos = len(y[y == 1])\n",
    "        num_neg = len(y[y == 0])\n",
    "        num_total = num_pos + num_neg\n",
    "\n",
    "        p_ratio = num_pos / num_total\n",
    "        n_ratio = num_neg / num_total\n",
    "\n",
    "        log_p = log(p_ratio, 2) if not p_ratio == 0 else 0\n",
    "        log_n = log(n_ratio, 2) if not n_ratio == 0 else 0\n",
    "\n",
    "        entropy = (-p_ratio * log_p) - (n_ratio * log_n)\n",
    "\n",
    "        return entropy, num_pos, num_neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    data = pd.read_csv(\"diabetes_data_upload.csv\")\n",
    "\n",
    "    data = encode_features(data)   \n",
    "\n",
    "    column_map = create_column_map(data.columns)\n",
    "       \n",
    "    data = np.array(data)\n",
    "\n",
    "    splitted = k_fold_cross_validation(data, 5)\n",
    "\n",
    "    return cross_validation(splitted, column_map)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************\n",
      "For Testing --> Fold 1 -> Precision: 0.96875, Recall: 0.9253731343283582, F1: 0.9465648854961832, Accuracy: 0.9326923076923077\n",
      "For Training --> Fold 1 -> Precision: 1.0, Recall: 0.9881422924901185, F1: 0.9940357852882703, Accuracy: 0.9927884615384616\n",
      "********************************************************\n",
      "For Testing --> Fold 2 -> Precision: 0.9538461538461539, Recall: 0.9393939393939394, F1: 0.9465648854961831, Accuracy: 0.9326923076923077\n",
      "For Training --> Fold 2 -> Precision: 0.996078431372549, Recall: 1.0, F1: 0.9980353634577603, Accuracy: 0.9975961538461539\n",
      "********************************************************\n",
      "For Testing --> Fold 3 -> Precision: 0.9508196721311475, Recall: 0.9666666666666667, F1: 0.9586776859504132, Accuracy: 0.9519230769230769\n",
      "For Training --> Fold 3 -> Precision: 0.9923664122137404, Recall: 1.0, F1: 0.9961685823754789, Accuracy: 0.9951923076923077\n",
      "********************************************************\n",
      "For Testing --> Fold 4 -> Precision: 0.9836065573770492, Recall: 0.967741935483871, F1: 0.975609756097561, Accuracy: 0.9711538461538461\n",
      "For Training --> Fold 4 -> Precision: 0.9885057471264368, Recall: 1.0, F1: 0.9942196531791908, Accuracy: 0.9927884615384616\n",
      "********************************************************\n",
      "For Testing --> Fold 5 -> Precision: 1.0, Recall: 0.9538461538461539, F1: 0.9763779527559054, Accuracy: 0.9711538461538461\n",
      "For Training --> Fold 5 -> Precision: 0.9883720930232558, Recall: 1.0, F1: 0.9941520467836257, Accuracy: 0.9927884615384616\n"
     ]
    }
   ],
   "source": [
    "trees, conf_matrices, column_map = main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in range(len(conf_matrices)):\n",
    "    print(f\"For Tree {index+1}:\")\n",
    "    display_confusion_matrix(conf_matrices[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run to see text based visualization for trees\n",
    "def display_trees(trees, column_map):\n",
    "    for index in range(len(trees)):\n",
    "        print(\"*****************************\")\n",
    "        print(f\"Tree {index+1}\")\n",
    "        tree = trees[index]\n",
    "        tree.display_tree(tree.head, column_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best model among 5 different splits is last split on most cases.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model works with an accuracy rate around 93%-96% even with test set. The high accuracy rate shows that our model works properly but for having better success metric, we have used precision, recall and f1 scores as stated in assignment pdf. According to high level of these metrics (can be seen above), we can say that our model works well even with test sets for 5 different splits. Also this indicates that the dataset has been carefully and consistently prepared and also shows that the number of outliers is low in our dataset (outliers can spoil model without pruning). \n",
    "\n",
    "Since we don't do pruning, our model is still slightly overfit (test-error > train-error), so post-pruning can increase metric scores even more. "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bb16ceb8f6ddb66c7208077f04fec0c796803b4ca5a28f5e811320302c8e18eb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
